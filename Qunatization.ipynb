{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPdXbr2org0Td0n+DhIWdXd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9f801db3db6547f390d8a8132baaf5f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f2bc2f7f161540e9b9a376ae92f71e46",
              "IPY_MODEL_f0a4781b0ba34f0db00db07e9692785d",
              "IPY_MODEL_625ed15384234daebdd03561ecca7a9b"
            ],
            "layout": "IPY_MODEL_37880ecd720f4d4496344018f4a2cea8"
          }
        },
        "f2bc2f7f161540e9b9a376ae92f71e46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f04fd9c06d8497c8f3764975ba44a6e",
            "placeholder": "​",
            "style": "IPY_MODEL_38fa787324fb40c282d373c7b5cfb8ba",
            "value": "Resolving data files: 100%"
          }
        },
        "f0a4781b0ba34f0db00db07e9692785d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d28552f310f498281ceed9a5c954283",
            "max": 1024,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7c9eb47a9c8143fcadff2ce531572896",
            "value": 1024
          }
        },
        "625ed15384234daebdd03561ecca7a9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89c45e11eb0c4ee385c53c8a69ce5cf1",
            "placeholder": "​",
            "style": "IPY_MODEL_ba640e96f17e4e97a02b49ba72c10e91",
            "value": " 1024/1024 [00:00&lt;00:00, 17962.08it/s]"
          }
        },
        "37880ecd720f4d4496344018f4a2cea8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f04fd9c06d8497c8f3764975ba44a6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38fa787324fb40c282d373c7b5cfb8ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d28552f310f498281ceed9a5c954283": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c9eb47a9c8143fcadff2ce531572896": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "89c45e11eb0c4ee385c53c8a69ce5cf1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba640e96f17e4e97a02b49ba72c10e91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cdbe9cbee5f54a4ebb50eb86f137e347": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_927a28bc56064791a9350e1a14a58099",
              "IPY_MODEL_e5eb280ac194436e8d5cf37c39b12708",
              "IPY_MODEL_28015671cfeb469598375fd9fa8da27b"
            ],
            "layout": "IPY_MODEL_5e5c96c83979430a9cb724081a0107b4"
          }
        },
        "927a28bc56064791a9350e1a14a58099": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6d1adf8c17d4e01ac742a5b37e0a7df",
            "placeholder": "​",
            "style": "IPY_MODEL_612f6dc38d814661a252b8e115a07d4e",
            "value": "Resolving data files: 100%"
          }
        },
        "e5eb280ac194436e8d5cf37c39b12708": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2031e4b8366f4ad8a36697dd0ee21950",
            "max": 1024,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_746b0d2a8945409381d083376ad543a6",
            "value": 1024
          }
        },
        "28015671cfeb469598375fd9fa8da27b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f3d83c795de494ca7eade84430751de",
            "placeholder": "​",
            "style": "IPY_MODEL_db81f8ef8b1b44eaa661f34bf8ba364f",
            "value": " 1024/1024 [00:01&lt;00:00, 637.25it/s]"
          }
        },
        "5e5c96c83979430a9cb724081a0107b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6d1adf8c17d4e01ac742a5b37e0a7df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "612f6dc38d814661a252b8e115a07d4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2031e4b8366f4ad8a36697dd0ee21950": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "746b0d2a8945409381d083376ad543a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0f3d83c795de494ca7eade84430751de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db81f8ef8b1b44eaa661f34bf8ba364f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Avinashhmavi/hack/blob/main/Qunatization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "B9U15f6uBDCK"
      },
      "outputs": [],
      "source": [
        "## LLM.int8()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def llm_int8_quantization(input_tensor: torch.Tensor,\n",
        "                          weights: torch.Tensor,\n",
        "                          alpha: float) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Quantizes the input and weight matrices using a mixed-precision scheme:\n",
        "      - Small magnitude features (columns where the max absolute value is <= alpha)\n",
        "        are quantized using vector-wise (row-wise for input and column-wise for weights)\n",
        "        quantization.\n",
        "      - Large magnitude features (columns with max abs > alpha) are computed in float16.\n",
        "    The results are then summed to obtain the final output.\n",
        "\n",
        "    Args:\n",
        "        input_tensor (torch.Tensor): Input tensor of shape (sequence_length, feature_dim)\n",
        "        weights (torch.Tensor): Weight matrix of shape (feature_dim, output_dim)\n",
        "        alpha (float): Threshold to distinguish between small and large magnitude columns.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Output tensor of shape (sequence_length, output_dim)\n",
        "    \"\"\"\n",
        "    # Determine which columns (features) are \"small\" vs. \"large\"\n",
        "    max_abs_per_col = torch.max(torch.abs(input_tensor), dim=0).values\n",
        "    small_mask = max_abs_per_col <= alpha\n",
        "    large_mask = max_abs_per_col > alpha\n",
        "    print(small_mask)\n",
        "    print(large_mask)\n",
        "\n",
        "    # Split the input and weight matrices accordingly\n",
        "    input_small = input_tensor[:, small_mask]\n",
        "    weights_small = weights[small_mask, :]\n",
        "\n",
        "    input_large = input_tensor[:, large_mask]\n",
        "    weights_large = weights[large_mask, :]\n",
        "\n",
        "    # For the input: perform row-wise quantization.\n",
        "    # Compute the row-wise (per sample) scale factor for the small features.\n",
        "    scale_input = torch.max(torch.abs(input_small), dim=1, keepdim=True).values  # Shape: (sequence_length, 1)\n",
        "    print(f\"Scale input: {scale_input.shape}\")\n",
        "    scale_input[scale_input == 0] = 1.0\n",
        "    input_small_quant = torch.round((input_small / scale_input) * 127).to(torch.int8)\n",
        "    print(f\"Input Small: {input_small_quant.shape}\")\n",
        "\n",
        "    # For the weights: perform column-wise quantization.\n",
        "    scale_weights = torch.max(torch.abs(weights_small), dim=0, keepdim=True).values  # Shape: (1, output_dim)\n",
        "    scale_weights[scale_weights == 0] = 1.0\n",
        "    weights_small_quant = torch.round((weights_small / scale_weights) * 127).to(torch.int8)\n",
        "    print(f\"Weight Small: {weights_small_quant.shape}\")\n",
        "\n",
        "    # Matrix multiplication with quantized values.\n",
        "    result_small_int32 = torch.matmul(input_small_quant.to(torch.int32),\n",
        "                                        weights_small_quant.to(torch.int32))\n",
        "    # Dequantize: the effective scale is the product of the input and weight scales,\n",
        "    result_small = result_small_int32.to(torch.float32) * (scale_input * scale_weights / (127 * 127))\n",
        "    print(f\"Result Small: {result_small.shape}\")\n",
        "\n",
        "    # Processing for Large Features\n",
        "    # For columns with large magnitude, we use float16 for higher precision.\n",
        "    input_large_fp16 = input_large.to(torch.float16)\n",
        "    print(f\"Input large: {input_large_fp16.shape}\")\n",
        "    weights_large_fp16 = weights_large.to(torch.float16)\n",
        "    print(f\"Weight large: {weights_large_fp16.shape}\")\n",
        "    result_large = torch.matmul(input_large_fp16, weights_large_fp16).to(torch.float32)\n",
        "\n",
        "    # Combine the Results\n",
        "    output = result_small + result_large\n",
        "    return output\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Dummy data: a sequence_length of 4 with 10 features and a weight matrix for 5 output neurons.\n",
        "    sequence_length, feature_dim, output_dim = 4, 10, 5\n",
        "    torch.manual_seed(0)\n",
        "    input_tensor = torch.randn(sequence_length, feature_dim)\n",
        "    weights = torch.randn(feature_dim, output_dim)\n",
        "    alpha = 1.5  # Threshold for distinguishing large vs. small magnitude features\n",
        "    output = llm_int8_quantization(input_tensor, weights, alpha)\n",
        "    output_complete = torch.matmul(input_tensor, weights)\n",
        "    #compute MSE b/w output and output_compelete\n",
        "    mse = torch.mean((output - output_complete) ** 2)\n",
        "    print(\"MSE: \", mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhKPkhP9BKoc",
        "outputId": "c1315b43-aca3-42e2-ada4-869b01995742"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ True, False,  True, False,  True,  True,  True, False, False, False])\n",
            "tensor([False,  True, False,  True, False, False, False,  True,  True,  True])\n",
            "Scale input: torch.Size([4, 1])\n",
            "Input Small: torch.Size([4, 5])\n",
            "Weight Small: torch.Size([5, 5])\n",
            "Result Small: torch.Size([4, 5])\n",
            "Input large: torch.Size([4, 5])\n",
            "Weight large: torch.Size([5, 5])\n",
            "MSE:  tensor(1.5778e-05)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVdK2xKPBfk6",
        "outputId": "eba903c8-1d71-4fc7-a673-aa9664a9e3c9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.5)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "try:\n",
        "    # Load model and tokenizer\n",
        "    model_id = 'gpt2'\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "    # Print model size\n",
        "    print(f\"Model size: {model.get_memory_footprint():,} bytes\")\n",
        "\n",
        "    # Check if CUDA is available for quantization\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"Quantization skipped: 8-bit quantization with bitsandbytes requires a CUDA-enabled GPU.\")\n",
        "        print(\"For CPU-only environments, consider using other optimization methods or a smaller model.\")\n",
        "    else:\n",
        "        try:\n",
        "            import bitsandbytes\n",
        "            # Configure 8-bit quantization\n",
        "            quantization_config = BitsAndBytesConfig(load_in_8bit=True, llm_int8_threshold=6.0)\n",
        "\n",
        "            # Load quantized model\n",
        "            model_int8 = AutoModelForCausalLM.from_pretrained(\n",
        "                model_id,\n",
        "                device_map='auto',\n",
        "                quantization_config=quantization_config\n",
        "            )\n",
        "            print(f\"Model size Quantized: {model_int8.get_memory_footprint():,} bytes\")\n",
        "\n",
        "        except ImportError:\n",
        "            print(\"Error: bitsandbytes package is not installed.\")\n",
        "            print(\"Please install it using: pip install -U bitsandbytes\")\n",
        "            print(\"Quantized model loading skipped.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcJETuN3BV5y",
        "outputId": "480ef0d8-015a-44b0-b76c-1b87ebf06ccd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model size: 510,342,192 bytes\n",
            "Model size Quantized: 176,527,896 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### GPTQ Algorithm"
      ],
      "metadata": {
        "id": "7SolPTrVDVii"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class HessianApproximator:\n",
        "    def __init__(self, hidden_dim):\n",
        "        \"\"\"\n",
        "        Initialize Hessian approximation matrix H.\n",
        "        Args:\n",
        "        - hidden_dim (int): Size of the input activation vectors.\n",
        "        \"\"\"\n",
        "        self.H = torch.zeros(hidden_dim, hidden_dim)  # Initialize Hessian approx\n",
        "        self.nsamples = 0  # Track number of samples seen\n",
        "\n",
        "    def update(self, activations):\n",
        "        \"\"\"\n",
        "        Update the Hessian approximation using a batch of activations.\n",
        "        Args:\n",
        "        - activations (torch.Tensor): Shape (batch_size, hidden_dim)\n",
        "        \"\"\"\n",
        "        batch_size = activations.shape[0]\n",
        "\n",
        "        # Normalize activation input\n",
        "        scale = torch.sqrt(torch.tensor(2.0 / (self.nsamples + batch_size)))\n",
        "        activations = scale * activations.float()\n",
        "\n",
        "        # Compute outer product\n",
        "        self.H *= self.nsamples / (self.nsamples + batch_size)  # Decay old information\n",
        "        self.H += activations.T @ activations  # Accumulate new information\n",
        "\n",
        "        # Update sample count\n",
        "        self.nsamples += batch_size\n",
        "\n",
        "    def get_hessian(self):\n",
        "        \"\"\"\n",
        "        Returns the approximated Hessian matrix.\n",
        "        \"\"\"\n",
        "        return self.H\n",
        "\n",
        "# Example Usage\n",
        "hidden_dim = 128  # Example hidden size\n",
        "hessian_approx = HessianApproximator(hidden_dim)\n",
        "\n",
        "# Simulated activations (batch of size 32)\n",
        "for _ in range(10):  # Simulate 10 batches\n",
        "    activations = torch.randn(32, hidden_dim)  # Random activations\n",
        "    hessian_approx.update(activations)\n",
        "print(hessian_approx.get_hessian())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gTTe_7nDupb",
        "outputId": "021cf63b-2ec3-428f-c8cb-7b367070afd7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.9952, -0.1386,  0.1125,  ..., -0.2000,  0.0804,  0.0931],\n",
            "        [-0.1386,  1.9721,  0.0205,  ...,  0.0940, -0.0426, -0.1067],\n",
            "        [ 0.1125,  0.0205,  1.9457,  ..., -0.1442, -0.0173,  0.0911],\n",
            "        ...,\n",
            "        [-0.2000,  0.0940, -0.1442,  ...,  2.1793, -0.0777,  0.0616],\n",
            "        [ 0.0804, -0.0426, -0.0173,  ..., -0.0777,  2.0420,  0.1914],\n",
            "        [ 0.0931, -0.1067,  0.0911,  ...,  0.0616,  0.1914,  1.8760]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Quantization"
      ],
      "metadata": {
        "id": "l5rIpUQwEAi-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def compute_hessian_inverse(H):\n",
        "    \"\"\"\n",
        "    Compute the inverse of the Hessian matrix with handling for numerical stability.\n",
        "\n",
        "    Args:\n",
        "        H (torch.Tensor): Approximated Hessian matrix (in_features x in_features).\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Inverse of the Hessian matrix.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Add small diagonal perturbation for numerical stability\n",
        "        epsilon = 1e-6\n",
        "        H_reg = H + torch.eye(H.shape[0], device=H.device) * epsilon\n",
        "        Hinv = torch.linalg.inv(H_reg)\n",
        "        return Hinv\n",
        "    except torch.linalg.LinAlgError:\n",
        "        # Fallback to pseudo-inverse if inversion fails\n",
        "        return torch.linalg.pinv(H)\n",
        "\n",
        "def zero_point_quantization(w, n_bits=8):\n",
        "    \"\"\"\n",
        "    Perform zero-point quantization on a weight vector.\n",
        "\n",
        "    Args:\n",
        "        w (torch.Tensor): Weight vector to quantize.\n",
        "        n_bits (int): Number of bits for quantization (default: 8).\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Quantized weight vector.\n",
        "    \"\"\"\n",
        "    # Compute min and max of the weights\n",
        "    w_min, w_max = w.min(), w.max()\n",
        "\n",
        "    # Calculate scale and zero point\n",
        "    q_min, q_max = -(2**(n_bits-1)), 2**(n_bits-1) - 1\n",
        "    scale = (w_max - w_min) / (q_max - q_min) if w_max != w_min else 1.0\n",
        "    zero_point = q_min - w_min / scale if scale != 0 else 0.0\n",
        "\n",
        "    # Quantize\n",
        "    q = torch.clamp(torch.round(w / scale + zero_point), q_min, q_max)\n",
        "\n",
        "    # Dequantize back to float\n",
        "    q = (q - zero_point) * scale\n",
        "    return q\n",
        "\n",
        "def iterative_quantization(layer, H, device='cpu'):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        layer (torch.nn.Module): The layer to be quantized. Its weight is expected to be 2D\n",
        "                                 with shape (out_features, in_features).\n",
        "        H (torch.Tensor): Approximated Hessian matrix (in_features x in_features).\n",
        "        device (str or torch.device): Device for computations (e.g., 'cpu' or 'cuda').\n",
        "\n",
        "    Returns:\n",
        "        torch.nn.Module: The same layer with its weights quantized.\n",
        "    \"\"\"\n",
        "    # Ensure device consistency\n",
        "    device = torch.device(device)\n",
        "    layer = layer.to(device)\n",
        "    H = H.to(device)\n",
        "\n",
        "    # Clone and convert the weight matrix to float\n",
        "    W = layer.weight.data.clone().float()  # Shape: (out_features, in_features)\n",
        "\n",
        "    # Handle potential \"dead\" weights by zeroing out columns where the Hessian diagonal is zero\n",
        "    dead = torch.diag(H) == 0\n",
        "    H[dead, dead] = 1\n",
        "    W[:, dead] = 0\n",
        "\n",
        "    # Compute the inverse Hessian for error compensation\n",
        "    Hinv = compute_hessian_inverse(H)\n",
        "\n",
        "    # Prepare an empty tensor for storing quantized weights\n",
        "    Q = torch.zeros_like(W)\n",
        "\n",
        "    # Number of columns (i.e., in_features)\n",
        "    n_cols = W.shape[1]\n",
        "\n",
        "    # Iteratively quantize each column\n",
        "    for i in range(n_cols):\n",
        "        # Get the current weight column\n",
        "        w = W[:, i]\n",
        "\n",
        "        # Quantize the column using the provided quantizer\n",
        "        q = zero_point_quantization(w)\n",
        "        Q[:, i] = q\n",
        "\n",
        "        # Use the Hessian inverse to compute a sensitivity factor\n",
        "        d = Hinv[i, i] if Hinv[i, i] != 0 else 1.0  # Avoid division by zero\n",
        "\n",
        "        # Compute the quantization error (scaled by the sensitivity)\n",
        "        error = (w - q) / d\n",
        "\n",
        "        # Adjust subsequent columns based on error compensation\n",
        "        if i < n_cols - 1:\n",
        "            # Hinv[i, i+1:] is a row vector containing sensitivity info for later columns\n",
        "            Hinv_damp = Hinv[i, i+1:] * 0.1\n",
        "            # Compute compensation: error (out_features,) needs to be applied to each subsequent column\n",
        "            # We outer product error with Hinv_damp to get a matrix of shape (out_features, n_cols-i-1)\n",
        "            compensation = torch.outer(error, Hinv_damp)\n",
        "            # Subtract the compensation from all subsequent columns\n",
        "            W[:, i+1:] = W[:, i+1:] - compensation\n",
        "\n",
        "    # Update the layer's weights with quantized values\n",
        "    layer.weight.data = Q\n",
        "\n",
        "    # Print the quantized weight shape for verification\n",
        "    print(\"Quantized weight shape:\", Q.shape)\n",
        "\n",
        "    return layer\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    # Create a sample linear layer\n",
        "    out_features, in_features = 10, 5\n",
        "    layer = nn.Linear(in_features, out_features)\n",
        "\n",
        "    # Create a synthetic Hessian matrix (positive semi-definite)\n",
        "    H = torch.randn(in_features, in_features)\n",
        "    H = H @ H.t() + torch.eye(in_features) * 0.1  # Ensure positive definite\n",
        "\n",
        "    # Call the quantization function\n",
        "    device = 'cpu'  # Use 'cuda' if GPU is available\n",
        "    quantized_layer = iterative_quantization(layer, H, device=device)\n",
        "\n",
        "    # Print the first few quantized weights for verification\n",
        "    print(\"Sample quantized weights:\\n\", quantized_layer.weight.data[:3, :3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wj1aotxMD1pf",
        "outputId": "eb4f7d34-c98f-49ea-b772-2cfc8d4d2990"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantized weight shape: torch.Size([10, 5])\n",
            "Sample quantized weights:\n",
            " tensor([[-0.0040,  0.2403, -0.3668],\n",
            "        [ 0.1191, -0.0077,  0.3558],\n",
            "        [-0.1362, -0.0887, -0.4272]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### quant"
      ],
      "metadata": {
        "id": "fsB-JoT3FUia"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsiFvsotFYut",
        "outputId": "4ddf90b6-106a-4eff-ab0c-604a49da40d9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nSJUCjPwE3MK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install auto-gptq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bu3waiIKGJ4t",
        "outputId": "b9d8d31a-e655-422a-fed7-b862ef4f3a41"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: auto-gptq in /usr/local/lib/python3.11/dist-packages (0.7.1+cu118)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from auto-gptq) (1.6.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from auto-gptq) (3.5.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from auto-gptq) (0.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from auto-gptq) (2.0.2)\n",
            "Requirement already satisfied: rouge in /usr/local/lib/python3.11/dist-packages (from auto-gptq) (1.0.1)\n",
            "Requirement already satisfied: gekko in /usr/local/lib/python3.11/dist-packages (from auto-gptq) (1.3.0)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from auto-gptq) (2.6.0+cu124)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from auto-gptq) (0.5.3)\n",
            "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.11/dist-packages (from auto-gptq) (4.51.3)\n",
            "Requirement already satisfied: peft>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from auto-gptq) (0.15.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from auto-gptq) (4.67.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->auto-gptq) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->auto-gptq) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->auto-gptq) (6.0.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->auto-gptq) (0.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->auto-gptq) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->auto-gptq) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->auto-gptq) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->auto-gptq) (0.21.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->auto-gptq) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->auto-gptq) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets->auto-gptq) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->auto-gptq) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets->auto-gptq) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets->auto-gptq) (3.11.15)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from rouge->auto-gptq) (1.17.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->auto-gptq) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->auto-gptq) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->auto-gptq) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->auto-gptq) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->auto-gptq) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->auto-gptq) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->auto-gptq) (1.20.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->auto-gptq) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->auto-gptq) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->auto-gptq) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->auto-gptq) (2025.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "try:\n",
        "    from datasets import load_dataset\n",
        "except ImportError:\n",
        "    print(\"Error: datasets package is not installed.\")\n",
        "    print(\"Please install it using: pip install datasets\")\n",
        "    print(\"Exiting due to missing dependency.\")\n",
        "    exit(1)\n",
        "\n",
        "try:\n",
        "    from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
        "except ImportError:\n",
        "    print(\"Error: auto_gptq package is not installed.\")\n",
        "    print(\"Please install it using: pip install auto-gptq\")\n",
        "    print(\"If you need GPU support with Triton, ensure you have a CUDA-enabled GPU and install the appropriate dependencies.\")\n",
        "    print(\"Exiting due to missing dependency.\")\n",
        "    exit(1)\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define base model and output directory\n",
        "model_id = \"gpt2\"\n",
        "out_dir = model_id + \"-GPTQ\"\n",
        "\n",
        "try:\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"Error: GPTQ quantization requires a CUDA-enabled GPU.\")\n",
        "        print(\"Quantization skipped. Please run this script on a machine with a compatible GPU.\")\n",
        "    else:\n",
        "        # Load quantize config, model, and tokenizer\n",
        "        quantize_config = BaseQuantizeConfig(\n",
        "            bits=4,\n",
        "            group_size=128,\n",
        "            damp_percent=0.01,\n",
        "            desc_act=False,\n",
        "        )\n",
        "        model = AutoGPTQForCausalLM.from_pretrained(model_id, quantize_config)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "        # Load data and tokenize examples\n",
        "        n_samples = 1024\n",
        "        data = load_dataset(\"allenai/c4\", data_files=\"en/c4-train.00001-of-01024.json.gz\", split=f\"train[:{n_samples*5}]\")\n",
        "        tokenized_data = tokenizer(\"\\n\\n\".join(data['text']), return_tensors='pt')\n",
        "\n",
        "        # Format tokenized examples\n",
        "        examples_ids = []\n",
        "        for _ in range(n_samples):\n",
        "            i = random.randint(0, tokenized_data.input_ids.shape[1] - tokenizer.model_max_length - 1)\n",
        "            j = i + tokenizer.model_max_length\n",
        "            input_ids = tokenized_data.input_ids[:, i:j]\n",
        "            attention_mask = torch.ones_like(input_ids)\n",
        "            examples_ids.append({'input_ids': input_ids, 'attention_mask': attention_mask})\n",
        "\n",
        "        # Quantize with GPTQ\n",
        "        model.quantize(\n",
        "            examples_ids,\n",
        "            batch_size=1,\n",
        "            use_triton=True,  # Triton is GPU-only, safe since we checked CUDA\n",
        "        )\n",
        "\n",
        "        # Save model and tokenizer\n",
        "        model.save_quantized(out_dir, use_safetensors=True)\n",
        "        tokenizer.save_pretrained(out_dir)\n",
        "        print(f\"Quantized model and tokenizer saved to {out_dir}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")\n",
        "    print(\"Please ensure all dependencies are installed and your environment supports the requested operations.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VK-_CQ-mFYYF",
        "outputId": "55252df8-43c7-4d07-e03a-29e4bd2d16df"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  @custom_bwd\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd(cast_inputs=torch.float16)\n",
            "WARNING:auto_gptq.nn_modules.qlinear.qlinear_cuda:CUDA extension not installed.\n",
            "WARNING:auto_gptq.nn_modules.qlinear.qlinear_cuda_old:CUDA extension not installed.\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2441065 > 1024). Running this sequence through the model will result in indexing errors\n",
            "INFO - Start quantizing layer 1/12\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 1/12\n",
            "INFO - Quantizing attn.c_attn in layer 1/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing attn.c_attn in layer 1/12...\n",
            "INFO - Quantizing attn.c_proj in layer 1/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing attn.c_proj in layer 1/12...\n",
            "INFO - Quantizing mlp.c_fc in layer 1/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.c_fc in layer 1/12...\n",
            "INFO - Quantizing mlp.c_proj in layer 1/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.c_proj in layer 1/12...\n",
            "INFO - Start quantizing layer 2/12\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 2/12\n",
            "INFO - Quantizing attn.c_attn in layer 2/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing attn.c_attn in layer 2/12...\n",
            "INFO - Quantizing attn.c_proj in layer 2/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing attn.c_proj in layer 2/12...\n",
            "INFO - Quantizing mlp.c_fc in layer 2/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.c_fc in layer 2/12...\n",
            "INFO - Quantizing mlp.c_proj in layer 2/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.c_proj in layer 2/12...\n",
            "INFO - Start quantizing layer 3/12\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 3/12\n",
            "INFO - Quantizing attn.c_attn in layer 3/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing attn.c_attn in layer 3/12...\n",
            "INFO - Quantizing attn.c_proj in layer 3/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing attn.c_proj in layer 3/12...\n",
            "INFO - Quantizing mlp.c_fc in layer 3/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.c_fc in layer 3/12...\n",
            "INFO - Quantizing mlp.c_proj in layer 3/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.c_proj in layer 3/12...\n",
            "INFO - Start quantizing layer 4/12\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 4/12\n",
            "INFO - Quantizing attn.c_attn in layer 4/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing attn.c_attn in layer 4/12...\n",
            "INFO - Quantizing attn.c_proj in layer 4/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing attn.c_proj in layer 4/12...\n",
            "INFO - Quantizing mlp.c_fc in layer 4/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.c_fc in layer 4/12...\n",
            "INFO - Quantizing mlp.c_proj in layer 4/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.c_proj in layer 4/12...\n",
            "INFO - Start quantizing layer 5/12\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 5/12\n",
            "INFO - Quantizing attn.c_attn in layer 5/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing attn.c_attn in layer 5/12...\n",
            "INFO - Quantizing attn.c_proj in layer 5/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing attn.c_proj in layer 5/12...\n",
            "INFO - Quantizing mlp.c_fc in layer 5/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.c_fc in layer 5/12...\n",
            "INFO - Quantizing mlp.c_proj in layer 5/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.c_proj in layer 5/12...\n",
            "INFO - Start quantizing layer 6/12\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 6/12\n",
            "INFO - Quantizing attn.c_attn in layer 6/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing attn.c_attn in layer 6/12...\n",
            "INFO - Quantizing attn.c_proj in layer 6/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing attn.c_proj in layer 6/12...\n",
            "INFO - Quantizing mlp.c_fc in layer 6/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.c_fc in layer 6/12...\n",
            "INFO - Quantizing mlp.c_proj in layer 6/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.c_proj in layer 6/12...\n",
            "INFO - Start quantizing layer 7/12\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 7/12\n",
            "INFO - Quantizing attn.c_attn in layer 7/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing attn.c_attn in layer 7/12...\n",
            "INFO - Quantizing attn.c_proj in layer 7/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing attn.c_proj in layer 7/12...\n",
            "INFO - Quantizing mlp.c_fc in layer 7/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.c_fc in layer 7/12...\n",
            "INFO - Quantizing mlp.c_proj in layer 7/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.c_proj in layer 7/12...\n",
            "INFO - Start quantizing layer 8/12\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 8/12\n",
            "INFO - Quantizing attn.c_attn in layer 8/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing attn.c_attn in layer 8/12...\n",
            "INFO - Quantizing attn.c_proj in layer 8/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing attn.c_proj in layer 8/12...\n",
            "INFO - Quantizing mlp.c_fc in layer 8/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.c_fc in layer 8/12...\n",
            "INFO - Quantizing mlp.c_proj in layer 8/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.c_proj in layer 8/12...\n",
            "INFO - Start quantizing layer 9/12\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 9/12\n",
            "INFO - Quantizing attn.c_attn in layer 9/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing attn.c_attn in layer 9/12...\n",
            "INFO - Quantizing attn.c_proj in layer 9/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing attn.c_proj in layer 9/12...\n",
            "INFO - Quantizing mlp.c_fc in layer 9/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.c_fc in layer 9/12...\n",
            "INFO - Quantizing mlp.c_proj in layer 9/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.c_proj in layer 9/12...\n",
            "INFO - Start quantizing layer 10/12\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 10/12\n",
            "INFO - Quantizing attn.c_attn in layer 10/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing attn.c_attn in layer 10/12...\n",
            "INFO - Quantizing attn.c_proj in layer 10/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing attn.c_proj in layer 10/12...\n",
            "INFO - Quantizing mlp.c_fc in layer 10/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.c_fc in layer 10/12...\n",
            "INFO - Quantizing mlp.c_proj in layer 10/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.c_proj in layer 10/12...\n",
            "INFO - Start quantizing layer 11/12\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 11/12\n",
            "INFO - Quantizing attn.c_attn in layer 11/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing attn.c_attn in layer 11/12...\n",
            "INFO - Quantizing attn.c_proj in layer 11/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing attn.c_proj in layer 11/12...\n",
            "INFO - Quantizing mlp.c_fc in layer 11/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.c_fc in layer 11/12...\n",
            "INFO - Quantizing mlp.c_proj in layer 11/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.c_proj in layer 11/12...\n",
            "INFO - Start quantizing layer 12/12\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 12/12\n",
            "INFO - Quantizing attn.c_attn in layer 12/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing attn.c_attn in layer 12/12...\n",
            "INFO - Quantizing attn.c_proj in layer 12/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing attn.c_proj in layer 12/12...\n",
            "INFO - Quantizing mlp.c_fc in layer 12/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.c_fc in layer 12/12...\n",
            "INFO - Quantizing mlp.c_proj in layer 12/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.c_proj in layer 12/12...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantized model and tokenizer saved to gpt2-GPTQ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Comparing models"
      ],
      "metadata": {
        "id": "OGTv9uevGNhC"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Sample text for perplexity calculation\n",
        "sample_text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "def calculate_perplexity(model, text, tokenizer):\n",
        "    \"\"\"\n",
        "    Calculate the perplexity of a model on given text.\n",
        "\n",
        "    Args:\n",
        "        model: The language model (e.g., GPT-2).\n",
        "        text (str): The input text to evaluate.\n",
        "        tokenizer: The tokenizer corresponding to the model.\n",
        "\n",
        "    Returns:\n",
        "        float: The perplexity score.\n",
        "    \"\"\"\n",
        "    # Encode the text\n",
        "    encodings = tokenizer(text, return_tensors='pt').to(device)\n",
        "    # Define input_ids and target_ids\n",
        "    input_ids = encodings.input_ids\n",
        "    target_ids = input_ids.clone()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, labels=target_ids)\n",
        "    # Loss calculation\n",
        "    neg_log_likelihood = outputs.loss\n",
        "    # Perplexity calculation\n",
        "    ppl = torch.exp(neg_log_likelihood)\n",
        "    return ppl\n",
        "\n",
        "try:\n",
        "    # Load tokenizer\n",
        "    model_id = \"gpt2\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "    # Load original GPT-2 model\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
        "    ppl = calculate_perplexity(model, sample_text, tokenizer)\n",
        "\n",
        "    # Handle INT8 quantized model\n",
        "    ppl_int8 = None\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"INT8 model loading skipped: bitsandbytes requires a CUDA-enabled GPU.\")\n",
        "    else:\n",
        "        try:\n",
        "            from transformers import BitsAndBytesConfig\n",
        "            quantization_config = BitsAndBytesConfig(load_in_8bit=True, llm_int8_threshold=6.0)\n",
        "            model_int8 = AutoModelForCausalLM.from_pretrained(\n",
        "                model_id,\n",
        "                device_map='auto',\n",
        "                quantization_config=quantization_config\n",
        "            )\n",
        "            ppl_int8 = calculate_perplexity(model_int8, sample_text, tokenizer)\n",
        "        except ImportError:\n",
        "            print(\"Error: bitsandbytes package is not installed for INT8 quantization.\")\n",
        "            print(\"Please install it using: pip install -U bitsandbytes\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading INT8 model: {str(e)}\")\n",
        "\n",
        "    # Handle GPTQ quantized model\n",
        "    ppl_gptq = None\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"GPTQ model loading skipped: Requires a CUDA-enabled GPU.\")\n",
        "    else:\n",
        "        try:\n",
        "            from auto_gptq import AutoGPTQForCausalLM\n",
        "            model_gptq = AutoGPTQForCausalLM.from_quantized(\n",
        "                model_id + \"-GPTQ\",\n",
        "                use_safetensors=True,\n",
        "                device_map='auto'\n",
        "            )\n",
        "            ppl_gptq = calculate_perplexity(model_gptq, sample_text, tokenizer)\n",
        "        except ImportError:\n",
        "            print(\"Error: auto_gptq package is not installed for GPTQ quantization.\")\n",
        "            print(\"Please install it using: pip install auto-gptq\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading GPTQ model: {str(e)}\")\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Original Model perplexity: {ppl.item():.2f}\")\n",
        "    if ppl_int8 is not None:\n",
        "        print(f\"INT8 perplexity:    {ppl_int8.item():.2f}\")\n",
        "    else:\n",
        "        print(\"INT8 perplexity:    Not calculated (missing GPU or dependencies)\")\n",
        "    if ppl_gptq is not None:\n",
        "        print(f\"GPTQ perplexity:  {ppl_gptq.item():.2f}\")\n",
        "    else:\n",
        "        print(\"GPTQ perplexity:  Not calculated (missing GPU or dependencies)\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")\n",
        "    print(\"Please ensure all dependencies are installed and models are correctly set up.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxRddbMtGjgR",
        "outputId": "767446b1-a827-4eed-eb1c-2fb83f1e93af"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
            "WARNING - Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\n",
            "WARNING:auto_gptq.modeling._base:Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\n",
            "WARNING - CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n",
            "1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n",
            "2. You are using pytorch without CUDA support.\n",
            "3. CUDA and nvcc are not installed in your device.\n",
            "WARNING:auto_gptq.modeling._base:CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n",
            "1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n",
            "2. You are using pytorch without CUDA support.\n",
            "3. CUDA and nvcc are not installed in your device.\n",
            "WARNING - ignoring unknown parameter in quantize_config.json: quant_method.\n",
            "WARNING:auto_gptq.modeling._base:ignoring unknown parameter in quantize_config.json: quant_method.\n",
            "INFO - The layer lm_head is not quantized.\n",
            "INFO:auto_gptq.modeling._base:The layer lm_head is not quantized.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Model perplexity: 162.47\n",
            "INT8 perplexity:    173.23\n",
            "GPTQ perplexity:  180.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### know the efficiency in quantized model"
      ],
      "metadata": {
        "id": "w8N4NhtlGkI4"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "n3_vLzjK0C6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "import os\n",
        "import psutil\n",
        "import numpy as np\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from auto_gptq import AutoGPTQForCausalLM\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define model paths\n",
        "model_id = \"gpt2\"\n",
        "quantized_model_dir = \"gpt2-GPTQ\"\n",
        "\n",
        "# Sample text for inference and perplexity\n",
        "sample_text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "max_new_tokens = 50  # Number of tokens to generate for inference speed test\n",
        "\n",
        "def get_model_size(model_dir):\n",
        "    \"\"\"Calculate the total size of model files in MB.\"\"\"\n",
        "    total_size = 0\n",
        "    for dirpath, _, filenames in os.walk(model_dir):\n",
        "        for f in filenames:\n",
        "            fp = os.path.join(dirpath, f)\n",
        "            total_size += os.path.getsize(fp)\n",
        "    return total_size / (1024 ** 2)  # Convert to MB\n",
        "\n",
        "def get_memory_usage():\n",
        "    \"\"\"Get current GPU memory usage in MB (if CUDA) or CPU memory in MB.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "        allocated = torch.cuda.memory_allocated() / (1024 ** 2)  # Convert to MB\n",
        "        reserved = torch.cuda.memory_reserved() / (1024 ** 2)  # Convert to MB\n",
        "        return allocated  # Use allocated memory for consistency\n",
        "    else:\n",
        "        process = psutil.Process()\n",
        "        return process.memory_info().rss / (1024 ** 2)  # Convert to MB\n",
        "\n",
        "def measure_inference_time(model, tokenizer, text, max_new_tokens, num_runs=10, is_gptq=False):\n",
        "    \"\"\"Measure average inference time for text generation.\"\"\"\n",
        "    # Tokenize input with attention mask\n",
        "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True).to(device)\n",
        "    input_ids = inputs['input_ids']\n",
        "    attention_mask = inputs['attention_mask']\n",
        "\n",
        "    # Warm-up run\n",
        "    with torch.no_grad():\n",
        "        if is_gptq:\n",
        "            # For GPTQ model, use generate with explicit kwargs\n",
        "            _ = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_new_tokens=max_new_tokens)\n",
        "        else:\n",
        "            _ = model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=max_new_tokens)\n",
        "\n",
        "    times = []\n",
        "    for _ in range(num_runs):\n",
        "        start_time = time.time()\n",
        "        with torch.no_grad():\n",
        "            if is_gptq:\n",
        "                _ = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_new_tokens=max_new_tokens)\n",
        "            else:\n",
        "                _ = model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=max_new_tokens)\n",
        "        times.append(time.time() - start_time)\n",
        "\n",
        "    return np.mean(times), np.std(times)\n",
        "\n",
        "def calculate_perplexity(model, text, tokenizer):\n",
        "    \"\"\"Calculate perplexity of the model on given text.\"\"\"\n",
        "    encodings = tokenizer(text, return_tensors='pt', padding=True, truncation=True).to(device)\n",
        "    input_ids = encodings['input_ids']\n",
        "    attention_mask = encodings['attention_mask']\n",
        "    target_ids = input_ids.clone()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=target_ids)\n",
        "    neg_log_likelihood = outputs.loss\n",
        "    ppl = torch.exp(neg_log_likelihood)\n",
        "    return ppl.item()\n",
        "\n",
        "try:\n",
        "    # Load tokenizer and set pad token\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    tokenizer.pad_token = tokenizer.eos_token  # Set pad token to eos token to avoid warnings\n",
        "\n",
        "    # Clear GPU memory before starting\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "    # --- Original Model ---\n",
        "    print(\"\\nLoading Original GPT-2 Model...\")\n",
        "    # Measure memory before loading\n",
        "    memory_before = get_memory_usage()\n",
        "\n",
        "    # Load model\n",
        "    model_original = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
        "\n",
        "    # Measure memory after loading\n",
        "    memory_after = get_memory_usage()\n",
        "    memory_usage_original = memory_after - memory_before\n",
        "    print(f\"Original Model Memory Usage: {memory_usage_original:.2f} MB\")\n",
        "\n",
        "    # Measure inference time\n",
        "    avg_time_original, std_time_original = measure_inference_time(\n",
        "        model_original, tokenizer, sample_text, max_new_tokens, is_gptq=False\n",
        "    )\n",
        "    print(f\"Original Model Inference Time: {avg_time_original:.4f} ± {std_time_original:.4f} seconds\")\n",
        "\n",
        "    # Calculate perplexity\n",
        "    ppl_original = calculate_perplexity(model_original, sample_text, tokenizer)\n",
        "    print(f\"Original Model Perplexity: {ppl_original:.2f}\")\n",
        "\n",
        "    # Clear GPU memory\n",
        "    del model_original\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "    # --- Quantized GPTQ Model ---\n",
        "    print(\"\\nLoading Quantized GPT-2 Model...\")\n",
        "    # Measure memory before loading\n",
        "    memory_before = get_memory_usage()\n",
        "\n",
        "    # Load quantized model\n",
        "    model_gptq = AutoGPTQForCausalLM.from_quantized(\n",
        "        quantized_model_dir,\n",
        "        use_safetensors=True,\n",
        "        device_map='auto'\n",
        "    )\n",
        "\n",
        "    # Measure memory after loading\n",
        "    memory_after = get_memory_usage()\n",
        "    memory_usage_gptq = memory_after - memory_before\n",
        "    print(f\"GPTQ Model Memory Usage: {memory_usage_gptq:.2f} MB\")\n",
        "\n",
        "    # Measure inference time\n",
        "    avg_time_gptq, std_time_gptq = measure_inference_time(\n",
        "        model_gptq, tokenizer, sample_text, max_new_tokens, is_gptq=True\n",
        "    )\n",
        "    print(f\"GPTQ Model Inference Time: {avg_time_gptq:.4f} ± {std_time_gptq:.4f} seconds\")\n",
        "\n",
        "    # Calculate perplexity\n",
        "    ppl_gptq = calculate_perplexity(model_gptq, sample_text, tokenizer)\n",
        "    print(f\"GPTQ Model Perplexity: {ppl_gptq:.2f}\")\n",
        "\n",
        "    # --- Model Size Comparison ---\n",
        "    original_model_size = get_model_size(model_id) if os.path.exists(model_id) else \"Unknown (download size)\"\n",
        "    quantized_model_size = get_model_size(quantized_model_dir)\n",
        "    print(f\"\\nOriginal Model Size: {original_model_size} MB\")\n",
        "    print(f\"Quantized Model Size: {quantized_model_size:.2f} MB\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")\n",
        "    print(\"Please ensure the quantized model exists in 'gpt2-GPTQ' and all dependencies are installed.\")\n",
        "\n",
        "finally:\n",
        "    # Clean up\n",
        "    if 'model_gptq' in locals():\n",
        "        del model_gptq\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zH10KJLB0EIb",
        "outputId": "41b5a6d3-33b2-4b59-fd49-0f3045b0e2f5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Loading Original GPT-2 Model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Model Memory Usage: 487.47 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "WARNING - Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\n",
            "WARNING:auto_gptq.modeling._base:Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\n",
            "WARNING - CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n",
            "1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n",
            "2. You are using pytorch without CUDA support.\n",
            "3. CUDA and nvcc are not installed in your device.\n",
            "WARNING:auto_gptq.modeling._base:CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n",
            "1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n",
            "2. You are using pytorch without CUDA support.\n",
            "3. CUDA and nvcc are not installed in your device.\n",
            "WARNING - ignoring unknown parameter in quantize_config.json: quant_method.\n",
            "WARNING:auto_gptq.modeling._base:ignoring unknown parameter in quantize_config.json: quant_method.\n",
            "INFO - The layer lm_head is not quantized.\n",
            "INFO:auto_gptq.modeling._base:The layer lm_head is not quantized.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Model Inference Time: 1.2108 ± 0.6665 seconds\n",
            "Original Model Perplexity: 162.47\n",
            "\n",
            "Loading Quantized GPT-2 Model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPTQ Model Memory Usage: -355.53 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPTQ Model Inference Time: 1.2667 ± 0.1180 seconds\n",
            "GPTQ Model Perplexity: 180.87\n",
            "\n",
            "Original Model Size: Unknown (download size) MB\n",
            "Quantized Model Size: 195.92 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "import os\n",
        "import psutil\n",
        "import numpy as np\n",
        "import subprocess\n",
        "import gc\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from auto_gptq import AutoGPTQForCausalLM\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define model paths\n",
        "model_id = \"gpt2\"\n",
        "quantized_model_dir = \"gpt2-GPTQ\"\n",
        "\n",
        "# Sample text for inference and perplexity\n",
        "sample_text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "max_new_tokens = 50  # Number of tokens to generate for inference speed test\n",
        "\n",
        "def get_model_size(model_dir):\n",
        "    \"\"\"Calculate the total size of model files in MB.\"\"\"\n",
        "    total_size = 0\n",
        "    for dirpath, _, filenames in os.walk(model_dir):\n",
        "        for f in filenames:\n",
        "            fp = os.path.join(dirpath, f)\n",
        "            total_size += os.path.getsize(fp)\n",
        "    return total_size / (1024 ** 2)  # Convert to MB\n",
        "\n",
        "def get_gpu_memory_usage():\n",
        "    \"\"\"Get current GPU memory usage in MB using nvidia-smi, with torch fallback.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        try:\n",
        "            result = subprocess.check_output(\n",
        "                [\"nvidia-smi\", \"--query-gpu=memory.used\", \"--format=csv,nounits,noheader\"],\n",
        "                encoding='utf-8'\n",
        "            )\n",
        "            time.sleep(0.1)  # Ensure measurement stability\n",
        "            return float(result.strip())  # Memory used in MB\n",
        "        except Exception as e:\n",
        "            print(f\"nvidia-smi failed: {e}, falling back to torch\")\n",
        "            torch.cuda.synchronize()\n",
        "            return torch.cuda.memory_allocated() / (1024 ** 2)\n",
        "    else:\n",
        "        process = psutil.Process()\n",
        "        return process.memory_info().rss / (1024 ** 2)\n",
        "\n",
        "def reset_gpu_memory():\n",
        "    \"\"\"Clear GPU memory and reset stats.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "        gc.collect()\n",
        "        torch.cuda.synchronize()\n",
        "        # Wait briefly to ensure memory is released\n",
        "        time.sleep(0.1)\n",
        "\n",
        "def measure_inference_time(model, tokenizer, text, max_new_tokens, num_runs=10, is_gptq=False):\n",
        "    \"\"\"Measure average inference time for text generation.\"\"\"\n",
        "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True).to(device)\n",
        "    input_ids = inputs['input_ids']\n",
        "    attention_mask = inputs['attention_mask']\n",
        "\n",
        "    # Warm-up run\n",
        "    with torch.no_grad():\n",
        "        if is_gptq:\n",
        "            _ = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_new_tokens=max_new_tokens, pad_token_id=tokenizer.pad_token_id)\n",
        "        else:\n",
        "            _ = model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=max_new_tokens, pad_token_id=tokenizer.pad_token_id)\n",
        "\n",
        "    times = []\n",
        "    for _ in range(num_runs):\n",
        "        start_time = time.time()\n",
        "        with torch.no_grad():\n",
        "            if is_gptq:\n",
        "                _ = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_new_tokens=max_new_tokens, pad_token_id=tokenizer.pad_token_id)\n",
        "            else:\n",
        "                _ = model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=max_new_tokens, pad_token_id=tokenizer.pad_token_id)\n",
        "        times.append(time.time() - start_time)\n",
        "\n",
        "    return np.mean(times), np.std(times)\n",
        "\n",
        "def calculate_perplexity(model, text, tokenizer):\n",
        "    \"\"\"Calculate perplexity of the model on given text.\"\"\"\n",
        "    encodings = tokenizer(text, return_tensors='pt', padding=True, truncation=True).to(device)\n",
        "    input_ids = encodings['input_ids']\n",
        "    attention_mask = encodings['attention_mask']\n",
        "    target_ids = input_ids.clone()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=target_ids)\n",
        "    neg_log_likelihood = outputs.loss\n",
        "    ppl = torch.exp(neg_log_likelihood)\n",
        "    return ppl.item()\n",
        "\n",
        "try:\n",
        "    # Load tokenizer and set pad token\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    tokenizer.pad_token = tokenizer.eos_token  # Set pad token to eos token\n",
        "\n",
        "    # --- Original Model ---\n",
        "    print(\"\\nLoading Original GPT-2 Model...\")\n",
        "    # Reset GPU memory\n",
        "    reset_gpu_memory()\n",
        "    # Measure memory before loading\n",
        "    memory_before = get_gpu_memory_usage()\n",
        "    print(f\"Memory before loading original: {memory_before:.2f} MB\")\n",
        "\n",
        "    # Load model\n",
        "    model_original = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
        "    torch.cuda.synchronize()  # Ensure model is fully loaded\n",
        "\n",
        "    # Measure memory after loading\n",
        "    memory_after = get_gpu_memory_usage()\n",
        "    memory_usage_original = memory_after - memory_before\n",
        "    print(f\"Original Model Memory Usage: {memory_usage_original:.2f} MB\")\n",
        "    print(f\"Memory after loading original: {memory_after:.2f} MB\")\n",
        "\n",
        "    # Measure inference time\n",
        "    avg_time_original, std_time_original = measure_inference_time(\n",
        "        model_original, tokenizer, sample_text, max_new_tokens, is_gptq=False\n",
        "    )\n",
        "    print(f\"Original Model Inference Time: {avg_time_original:.4f} ± {std_time_original:.4f} seconds\")\n",
        "\n",
        "    # Calculate perplexity\n",
        "    ppl_original = calculate_perplexity(model_original, sample_text, tokenizer)\n",
        "    print(f\"Original Model Perplexity: {ppl_original:.2f}\")\n",
        "\n",
        "    # Clear GPU memory\n",
        "    del model_original\n",
        "    reset_gpu_memory()\n",
        "    memory_after_clear = get_gpu_memory_usage()\n",
        "    print(f\"Memory after clearing original: {memory_after_clear:.2f} MB\")\n",
        "\n",
        "    # --- Quantized GPTQ Model ---\n",
        "    print(\"\\nLoading Quantized GPT-2 Model...\")\n",
        "    # Reset GPU memory\n",
        "    reset_gpu_memory()\n",
        "    # Measure memory before loading\n",
        "    memory_before = get_gpu_memory_usage()\n",
        "    print(f\"Memory before loading GPTQ: {memory_before:.2f} MB\")\n",
        "\n",
        "    # Load quantized model\n",
        "    model_gptq = AutoGPTQForCausalLM.from_quantized(\n",
        "        quantized_model_dir,\n",
        "        use_safetensors=True,\n",
        "        device_map='auto'\n",
        "    )\n",
        "    torch.cuda.synchronize()  # Ensure model is fully loaded\n",
        "\n",
        "    # Measure memory after loading\n",
        "    memory_after = get_gpu_memory_usage()\n",
        "    memory_usage_gptq = memory_after - memory_before\n",
        "    print(f\"GPTQ Model Memory Usage: {memory_usage_gptq:.2f} MB\")\n",
        "    print(f\"Memory after loading GPTQ: {memory_after:.2f} MB\")\n",
        "\n",
        "    # Measure inference time\n",
        "    avg_time_gptq, std_time_gptq = measure_inference_time(\n",
        "        model_gptq, tokenizer, sample_text, max_new_tokens, is_gptq=True\n",
        "    )\n",
        "    print(f\"GPTQ Model Inference Time: {avg_time_gptq:.4f} ± {std_time_gptq:.4f} seconds\")\n",
        "\n",
        "    # Calculate perplexity\n",
        "    ppl_gptq = calculate_perplexity(model_gptq, sample_text, tokenizer)\n",
        "    print(f\"GPTQ Model Perplexity: {ppl_gptq:.2f}\")\n",
        "\n",
        "    # --- Model Size Comparison ---\n",
        "    original_model_size = get_model_size(model_id) if os.path.exists(model_id) else \"Unknown (download size)\"\n",
        "    quantized_model_size = get_model_size(quantized_model_dir)\n",
        "    print(f\"\\nOriginal Model Size: {original_model_size} MB\")\n",
        "    print(f\"Quantized Model Size: {quantized_model_size:.2f} MB\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")\n",
        "    print(\"Please ensure the quantized model exists in 'gpt2-GPTQ' and all dependencies are installed.\")\n",
        "\n",
        "finally:\n",
        "    # Clean up\n",
        "    if 'model_gptq' in locals():\n",
        "        del model_gptq\n",
        "    reset_gpu_memory()\n",
        "    print(f\"Memory after final cleanup: {get_gpu_memory_usage():.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjAYvA0D2QsX",
        "outputId": "31227b07-e40d-435a-aaa2-707d66658f6d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Loading Original GPT-2 Model...\n",
            "Memory before loading original: 1114.00 MB\n",
            "Original Model Memory Usage: 276.00 MB\n",
            "Memory after loading original: 1390.00 MB\n",
            "Original Model Inference Time: 0.4738 ± 0.0067 seconds\n",
            "Original Model Perplexity: 162.47\n",
            "Memory after clearing original: 1390.00 MB\n",
            "\n",
            "Loading Quantized GPT-2 Model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING - Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\n",
            "WARNING:auto_gptq.modeling._base:Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\n",
            "WARNING - CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n",
            "1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n",
            "2. You are using pytorch without CUDA support.\n",
            "3. CUDA and nvcc are not installed in your device.\n",
            "WARNING:auto_gptq.modeling._base:CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n",
            "1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n",
            "2. You are using pytorch without CUDA support.\n",
            "3. CUDA and nvcc are not installed in your device.\n",
            "WARNING - ignoring unknown parameter in quantize_config.json: quant_method.\n",
            "WARNING:auto_gptq.modeling._base:ignoring unknown parameter in quantize_config.json: quant_method.\n",
            "INFO - The layer lm_head is not quantized.\n",
            "INFO:auto_gptq.modeling._base:The layer lm_head is not quantized.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory before loading GPTQ: 1114.00 MB\n",
            "GPTQ Model Memory Usage: 18.00 MB\n",
            "Memory after loading GPTQ: 1132.00 MB\n",
            "GPTQ Model Inference Time: 1.2407 ± 0.1117 seconds\n",
            "GPTQ Model Perplexity: 180.87\n",
            "\n",
            "Original Model Size: Unknown (download size) MB\n",
            "Quantized Model Size: 195.92 MB\n",
            "Memory after final cleanup: 1132.00 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from auto_gptq import AutoGPTQForCausalLM\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define model and tokenizer paths\n",
        "model_id = \"gpt2\"\n",
        "quantized_model_dir = \"gpt2-GPTQ\"\n",
        "\n",
        "# Define prompts to test the model\n",
        "prompts = [\n",
        "    \"What is the capital of France, and what is its largest city?\",\n",
        "    \"Write a short story about a robot exploring an abandoned city.\",\n",
        "    \"If a car travels 60 miles in 1 hour, how far will it travel in 2.5 hours?\",\n",
        "    \"Explain why the sky appears blue.\",\n",
        "    \"What’s your favorite book, and why?\"\n",
        "]\n",
        "\n",
        "def query_model(model, tokenizer, prompt, max_new_tokens=100, temperature=0.7, top_p=0.9):\n",
        "    \"\"\"Query the model with a prompt and return the generated response.\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(device)\n",
        "    input_ids = inputs['input_ids']\n",
        "    attention_mask = inputs['attention_mask']\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            do_sample=True  # Enable sampling for varied responses\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response.strip()\n",
        "\n",
        "try:\n",
        "    # Load tokenizer and set pad token\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    tokenizer.pad_token = tokenizer.eos_token  # Avoid pad token warnings\n",
        "\n",
        "    # Load quantized model\n",
        "    print(\"\\nLoading Quantized GPT-2 Model...\")\n",
        "    model_gptq = AutoGPTQForCausalLM.from_quantized(\n",
        "        quantized_model_dir,\n",
        "        use_safetensors=True,\n",
        "        device_map='auto'\n",
        "    )\n",
        "\n",
        "    # Query the model with each prompt\n",
        "    print(\"\\nQuerying Quantized GPT-2 Model...\")\n",
        "    for i, prompt in enumerate(prompts, 1):\n",
        "        print(f\"\\nPrompt {i}: {prompt}\")\n",
        "        response = query_model(model_gptq, tokenizer, prompt)\n",
        "        print(f\"Response: {response}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")\n",
        "    print(\"Please ensure the quantized model exists in 'gpt2-GPTQ' and all dependencies are installed.\")\n",
        "\n",
        "finally:\n",
        "    # Clean up\n",
        "    if 'model_gptq' in locals():\n",
        "        del model_gptq\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-JbhhsM2tlK",
        "outputId": "44e19c84-13f5-459f-f596-c2a1a7b040df"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING - Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\n",
            "WARNING:auto_gptq.modeling._base:Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\n",
            "WARNING - CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n",
            "1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n",
            "2. You are using pytorch without CUDA support.\n",
            "3. CUDA and nvcc are not installed in your device.\n",
            "WARNING:auto_gptq.modeling._base:CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n",
            "1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n",
            "2. You are using pytorch without CUDA support.\n",
            "3. CUDA and nvcc are not installed in your device.\n",
            "WARNING - ignoring unknown parameter in quantize_config.json: quant_method.\n",
            "WARNING:auto_gptq.modeling._base:ignoring unknown parameter in quantize_config.json: quant_method.\n",
            "INFO - The layer lm_head is not quantized.\n",
            "INFO:auto_gptq.modeling._base:The layer lm_head is not quantized.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading Quantized GPT-2 Model...\n",
            "\n",
            "Querying Quantized GPT-2 Model...\n",
            "\n",
            "Prompt 1: What is the capital of France, and what is its largest city?\n",
            "Response: What is the capital of France, and what is its largest city? How is it known, and why is it known, in France? And how does it look?\n",
            "\n",
            "The French capital is the capital of France. It is the capital of the French Republic, France. It is the capital of France. It is the capital of the French republic. It is the capital of France. It is the capital of France.\n",
            "\n",
            "Let us then look at the French capital of the United States. The United States is the capital of the United States. It is\n",
            "\n",
            "Prompt 2: Write a short story about a robot exploring an abandoned city.\n",
            "Response: Write a short story about a robot exploring an abandoned city. A group of young robots is trying to find the perfect place to live. The next day, they find a small town where there is no city, and they discover a city of the robots. They decide to live in a city of the robots.\n",
            "\n",
            "This is not a novel, this is a story of people in a world where they can be themselves, but also to be robots.\n",
            "\n",
            "This is not a novel, this is a story of people living in a world where robots are\n",
            "\n",
            "Prompt 3: If a car travels 60 miles in 1 hour, how far will it travel in 2.5 hours?\n",
            "Response: If a car travels 60 miles in 1 hour, how far will it travel in 2.5 hours?\n",
            "\n",
            "You can calculate your mileage in a variety of ways, including distance traveled on a regular basis. This calculator will help you figure out your daily mileage, how many miles you have travelled, how many miles you have traveled on a regular basis, and how long you have been driving. It also allows you to find the number of miles you've driven on a regular basis.\n",
            "\n",
            "To calculate your daily mileage, you need to find the number of miles you have travelled on a regular basis.\n",
            "\n",
            "Prompt 4: Explain why the sky appears blue.\n",
            "Response: Explain why the sky appears blue.\n",
            "\n",
            "For the record, it is the blue of the sun that gives the blue sky.\n",
            "\n",
            "For the record, it is the blue of the sky that gives the blue sky.\n",
            "\n",
            "The sky is blue, right?\n",
            "\n",
            "The sky is blue, right?\n",
            "\n",
            "It's not that blue, it's not that blue, it's just that blue.\n",
            "\n",
            "The sky is blue, right?\n",
            "\n",
            "The sky is blue, right?\n",
            "\n",
            "The sky is blue\n",
            "\n",
            "Prompt 5: What’s your favorite book, and why?\n",
            "Response: What’s your favorite book, and why?\n",
            "\n",
            "I was reading about the life of William Shakespeare in the early 16th century. I read his novels and I think that's what I was most interested in. I was curious, and I found out that his life was interesting and that he was a very clever man. So, I went to the library and read the books. I had a book of Shakespeare's \"Hymn of the Monkey\" and that was the most interesting part of his life.\n",
            "\n",
            "So, how did\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import shutil\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define model and output paths\n",
        "model_id = \"gpt2\"\n",
        "quantized_model_dir = \"gpt2-GPTQ\"\n",
        "\n",
        "# Example texts for quantization calibration\n",
        "def get_calibration_examples(num_examples=128):\n",
        "    \"\"\"Load example texts from C4 English dataset for quantization.\"\"\"\n",
        "    dataset = load_dataset(\"allenai/c4\", \"en\", split=\"train\", streaming=True)\n",
        "    examples = []\n",
        "    for i, example in enumerate(dataset):\n",
        "        if i >= num_examples:\n",
        "            break\n",
        "        text = example['text'][:512]  # Limit to 512 characters\n",
        "        examples.append(text)\n",
        "    return examples\n",
        "\n",
        "try:\n",
        "    # Check if quantized_model_dir exists and remove it\n",
        "    if os.path.exists(quantized_model_dir):\n",
        "        print(f\"\\nDirectory '{quantized_model_dir}' already exists. Deleting to create a fresh quantized model...\")\n",
        "        shutil.rmtree(quantized_model_dir)\n",
        "\n",
        "    # Load tokenizer and model\n",
        "    print(\"\\nLoading GPT-2 Model and Tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    tokenizer.pad_token = tokenizer.eos_token  # Avoid pad token warnings\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
        "\n",
        "    # Prepare quantization configuration\n",
        "    quantize_config = BaseQuantizeConfig(\n",
        "        bits=4,          # 4-bit quantization\n",
        "        group_size=128,  # Group size for quantization\n",
        "        damp_percent=0.01,  # Damping factor\n",
        "        desc_act=False   # Disable act-order for stability\n",
        "    )\n",
        "\n",
        "    # Get calibration examples\n",
        "    print(\"\\nLoading calibration examples...\")\n",
        "    examples = get_calibration_examples()\n",
        "\n",
        "    # Tokenize examples\n",
        "    tokenized_examples = [tokenizer(ex, return_tensors='pt', padding=True, truncation=True).to(device) for ex in examples]\n",
        "\n",
        "    # Quantize the model\n",
        "    print(\"\\nQuantizing GPT-2 Model...\")\n",
        "    quantized_model = AutoGPTQForCausalLM.from_pretrained(model_id, quantize_config, device_map='cuda')\n",
        "    quantized_model.quantize(tokenized_examples, use_triton=False)  # Triton may require CUDA kernels\n",
        "\n",
        "    # Save quantized model\n",
        "    print(f\"\\nSaving quantized model to '{quantized_model_dir}'...\")\n",
        "    quantized_model.save_quantized(quantized_model_dir, use_safetensors=True, safetensors_metadata={'format': 'pt'})\n",
        "\n",
        "    # Save tokenizer files\n",
        "    print(f\"\\nSaving tokenizer to '{quantized_model_dir}'...\")\n",
        "    tokenizer.save_pretrained(quantized_model_dir)\n",
        "\n",
        "    # Rename safetensors file to match expected name\n",
        "    old_safetensors = os.path.join(quantized_model_dir, 'gptq_model-4bit-128g.safetensors')\n",
        "    new_safetensors = os.path.join(quantized_model_dir, 'model.safetensors')\n",
        "    if os.path.exists(old_safetensors):\n",
        "        os.rename(old_safetensors, new_safetensors)\n",
        "        print(f\"Renamed '{old_safetensors}' to '{new_safetensors}' for compatibility.\")\n",
        "\n",
        "    # Verify saved files\n",
        "    print(\"\\nVerifying saved files...\")\n",
        "    required_files = ['model.safetensors', 'config.json', 'tokenizer.json', 'vocab.json', 'merges.txt']\n",
        "    saved_files = os.listdir(quantized_model_dir)\n",
        "    for f in required_files:\n",
        "        if f in saved_files:\n",
        "            print(f\"Found: {f}\")\n",
        "        else:\n",
        "            print(f\"Missing: {f}\")\n",
        "            raise FileNotFoundError(f\"Failed to save {f} in '{quantized_model_dir}'\")\n",
        "\n",
        "    print(f\"\\nQuantization complete. Model saved to '{quantized_model_dir}'.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")\n",
        "    print(\"Please ensure all dependencies are installed and the model ID is correct.\")\n",
        "\n",
        "finally:\n",
        "    # Clean up\n",
        "    if 'model' in locals():\n",
        "        del model\n",
        "    if 'quantized_model' in locals():\n",
        "        del quantized_model\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        import gc\n",
        "        gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "9f801db3db6547f390d8a8132baaf5f2",
            "f2bc2f7f161540e9b9a376ae92f71e46",
            "f0a4781b0ba34f0db00db07e9692785d",
            "625ed15384234daebdd03561ecca7a9b",
            "37880ecd720f4d4496344018f4a2cea8",
            "1f04fd9c06d8497c8f3764975ba44a6e",
            "38fa787324fb40c282d373c7b5cfb8ba",
            "0d28552f310f498281ceed9a5c954283",
            "7c9eb47a9c8143fcadff2ce531572896",
            "89c45e11eb0c4ee385c53c8a69ce5cf1",
            "ba640e96f17e4e97a02b49ba72c10e91",
            "cdbe9cbee5f54a4ebb50eb86f137e347",
            "927a28bc56064791a9350e1a14a58099",
            "e5eb280ac194436e8d5cf37c39b12708",
            "28015671cfeb469598375fd9fa8da27b",
            "5e5c96c83979430a9cb724081a0107b4",
            "d6d1adf8c17d4e01ac742a5b37e0a7df",
            "612f6dc38d814661a252b8e115a07d4e",
            "2031e4b8366f4ad8a36697dd0ee21950",
            "746b0d2a8945409381d083376ad543a6",
            "0f3d83c795de494ca7eade84430751de",
            "db81f8ef8b1b44eaa661f34bf8ba364f"
          ]
        },
        "id": "4enXq_-k4PJu",
        "outputId": "1b8bcfcb-0ce5-4f81-a8b2-79e2aa9aa99c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Directory 'gpt2-GPTQ' already exists. Deleting to create a fresh quantized model...\n",
            "\n",
            "Loading GPT-2 Model and Tokenizer...\n",
            "\n",
            "Loading calibration examples...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9f801db3db6547f390d8a8132baaf5f2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cdbe9cbee5f54a4ebb50eb86f137e347"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Quantizing GPT-2 Model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO - Start quantizing layer 1/12\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 1/12\n",
            "INFO - Quantizing attn.c_attn in layer 1/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing attn.c_attn in layer 1/12...\n",
            "INFO - Quantizing attn.c_proj in layer 1/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing attn.c_proj in layer 1/12...\n",
            "INFO - Quantizing mlp.c_fc in layer 1/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.c_fc in layer 1/12...\n",
            "INFO - Quantizing mlp.c_proj in layer 1/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.c_proj in layer 1/12...\n",
            "INFO - Start quantizing layer 2/12\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 2/12\n",
            "INFO - Quantizing attn.c_attn in layer 2/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing attn.c_attn in layer 2/12...\n",
            "INFO - Quantizing attn.c_proj in layer 2/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing attn.c_proj in layer 2/12...\n",
            "INFO - Quantizing mlp.c_fc in layer 2/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.c_fc in layer 2/12...\n",
            "INFO - Quantizing mlp.c_proj in layer 2/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.c_proj in layer 2/12...\n",
            "INFO - Start quantizing layer 3/12\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 3/12\n",
            "INFO - Quantizing attn.c_attn in layer 3/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing attn.c_attn in layer 3/12...\n",
            "INFO - Quantizing attn.c_proj in layer 3/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing attn.c_proj in layer 3/12...\n",
            "INFO - Quantizing mlp.c_fc in layer 3/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.c_fc in layer 3/12...\n",
            "INFO - Quantizing mlp.c_proj in layer 3/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.c_proj in layer 3/12...\n",
            "INFO - Start quantizing layer 4/12\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 4/12\n",
            "INFO - Quantizing attn.c_attn in layer 4/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing attn.c_attn in layer 4/12...\n",
            "INFO - Quantizing attn.c_proj in layer 4/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing attn.c_proj in layer 4/12...\n",
            "INFO - Quantizing mlp.c_fc in layer 4/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.c_fc in layer 4/12...\n",
            "INFO - Quantizing mlp.c_proj in layer 4/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.c_proj in layer 4/12...\n",
            "INFO - Start quantizing layer 5/12\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 5/12\n",
            "INFO - Quantizing attn.c_attn in layer 5/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing attn.c_attn in layer 5/12...\n",
            "INFO - Quantizing attn.c_proj in layer 5/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing attn.c_proj in layer 5/12...\n",
            "INFO - Quantizing mlp.c_fc in layer 5/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.c_fc in layer 5/12...\n",
            "INFO - Quantizing mlp.c_proj in layer 5/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.c_proj in layer 5/12...\n",
            "INFO - Start quantizing layer 6/12\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 6/12\n",
            "INFO - Quantizing attn.c_attn in layer 6/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing attn.c_attn in layer 6/12...\n",
            "INFO - Quantizing attn.c_proj in layer 6/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing attn.c_proj in layer 6/12...\n",
            "INFO - Quantizing mlp.c_fc in layer 6/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.c_fc in layer 6/12...\n",
            "INFO - Quantizing mlp.c_proj in layer 6/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.c_proj in layer 6/12...\n",
            "INFO - Start quantizing layer 7/12\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 7/12\n",
            "INFO - Quantizing attn.c_attn in layer 7/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing attn.c_attn in layer 7/12...\n",
            "INFO - Quantizing attn.c_proj in layer 7/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing attn.c_proj in layer 7/12...\n",
            "INFO - Quantizing mlp.c_fc in layer 7/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.c_fc in layer 7/12...\n",
            "INFO - Quantizing mlp.c_proj in layer 7/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.c_proj in layer 7/12...\n",
            "INFO - Start quantizing layer 8/12\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 8/12\n",
            "INFO - Quantizing attn.c_attn in layer 8/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing attn.c_attn in layer 8/12...\n",
            "INFO - Quantizing attn.c_proj in layer 8/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing attn.c_proj in layer 8/12...\n",
            "INFO - Quantizing mlp.c_fc in layer 8/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.c_fc in layer 8/12...\n",
            "INFO - Quantizing mlp.c_proj in layer 8/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.c_proj in layer 8/12...\n",
            "INFO - Start quantizing layer 9/12\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 9/12\n",
            "INFO - Quantizing attn.c_attn in layer 9/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing attn.c_attn in layer 9/12...\n",
            "INFO - Quantizing attn.c_proj in layer 9/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing attn.c_proj in layer 9/12...\n",
            "INFO - Quantizing mlp.c_fc in layer 9/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.c_fc in layer 9/12...\n",
            "INFO - Quantizing mlp.c_proj in layer 9/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.c_proj in layer 9/12...\n",
            "INFO - Start quantizing layer 10/12\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 10/12\n",
            "INFO - Quantizing attn.c_attn in layer 10/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing attn.c_attn in layer 10/12...\n",
            "INFO - Quantizing attn.c_proj in layer 10/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing attn.c_proj in layer 10/12...\n",
            "INFO - Quantizing mlp.c_fc in layer 10/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.c_fc in layer 10/12...\n",
            "INFO - Quantizing mlp.c_proj in layer 10/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.c_proj in layer 10/12...\n",
            "INFO - Start quantizing layer 11/12\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 11/12\n",
            "INFO - Quantizing attn.c_attn in layer 11/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing attn.c_attn in layer 11/12...\n",
            "INFO - Quantizing attn.c_proj in layer 11/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing attn.c_proj in layer 11/12...\n",
            "INFO - Quantizing mlp.c_fc in layer 11/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.c_fc in layer 11/12...\n",
            "INFO - Quantizing mlp.c_proj in layer 11/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.c_proj in layer 11/12...\n",
            "INFO - Start quantizing layer 12/12\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 12/12\n",
            "INFO - Quantizing attn.c_attn in layer 12/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing attn.c_attn in layer 12/12...\n",
            "INFO - Quantizing attn.c_proj in layer 12/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing attn.c_proj in layer 12/12...\n",
            "INFO - Quantizing mlp.c_fc in layer 12/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.c_fc in layer 12/12...\n",
            "INFO - Quantizing mlp.c_proj in layer 12/12...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.c_proj in layer 12/12...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saving quantized model to 'gpt2-GPTQ'...\n",
            "\n",
            "Saving tokenizer to 'gpt2-GPTQ'...\n",
            "Renamed 'gpt2-GPTQ/gptq_model-4bit-128g.safetensors' to 'gpt2-GPTQ/model.safetensors' for compatibility.\n",
            "\n",
            "Verifying saved files...\n",
            "Found: model.safetensors\n",
            "Found: config.json\n",
            "Found: tokenizer.json\n",
            "Found: vocab.json\n",
            "Found: merges.txt\n",
            "\n",
            "Quantization complete. Model saved to 'gpt2-GPTQ'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls -l gpt2-GPTQ"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-Yj1F104thP",
        "outputId": "2f4efbe7-64fa-49bb-fd20-9a5a1ee09717"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 200640\n",
            "-rw-r--r-- 1 root root      1237 May  5 19:07 config.json\n",
            "-rw-r--r-- 1 root root 200619312 May  5 19:07 gptq_model-4bit-128g.safetensors\n",
            "-rw-r--r-- 1 root root    456318 May  5 19:07 merges.txt\n",
            "-rw-r--r-- 1 root root       266 May  5 19:07 quantize_config.json\n",
            "-rw-r--r-- 1 root root        99 May  5 19:07 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root       475 May  5 19:07 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root   3557680 May  5 19:07 tokenizer.json\n",
            "-rw-r--r-- 1 root root    798156 May  5 19:07 vocab.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import time\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from auto_gptq import AutoGPTQForCausalLM\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define model and tokenizer paths\n",
        "model_id = \"gpt2\"\n",
        "quantized_model_dir = \"gpt2-GPTQ\"\n",
        "\n",
        "# Define prompts to test the model\n",
        "prompts = [\n",
        "    \"What is the capital of France, and what is its largest city?\",\n",
        "    \"Write a short story about a robot exploring an abandoned city.\",\n",
        "    \"If a car travels 60 miles in 1 hour, how far will it travel in 2.5 hours?\",\n",
        "    \"Explain why the sky appears blue.\",\n",
        "    \"What’s your favorite book, and why?\"\n",
        "]\n",
        "\n",
        "def verify_model_directory(model_dir):\n",
        "    \"\"\"Verify that the model directory contains required files.\"\"\"\n",
        "    required_files = ['model.safetensors', 'config.json', 'tokenizer.json', 'vocab.json', 'merges.txt']\n",
        "    return all(os.path.exists(os.path.join(model_dir, f)) for f in required_files)\n",
        "\n",
        "def query_model(model, tokenizer, prompt, max_new_tokens=100, num_beams=5, temperature=0.5):\n",
        "    \"\"\"Query the model with a prompt and return the generated response with inference time.\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(device)\n",
        "    input_ids = inputs['input_ids']\n",
        "    attention_mask = inputs['attention_mask']\n",
        "\n",
        "    start_time = time.perf_counter()\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            num_beams=num_beams,\n",
        "            temperature=temperature,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            early_stopping=True,\n",
        "            no_repeat_ngram_size=2  # Prevent repetition\n",
        "        )\n",
        "    inference_time = time.perf_counter() - start_time\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response.strip(), inference_time\n",
        "\n",
        "try:\n",
        "    # Load tokenizer and set pad token\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    tokenizer.pad_token = tokenizer.eos_token  # Avoid pad token warnings\n",
        "\n",
        "    # Verify quantized model directory\n",
        "    if not verify_model_directory(quantized_model_dir):\n",
        "        raise FileNotFoundError(f\"Quantized model directory '{quantized_model_dir}' is missing required files.\")\n",
        "\n",
        "    # Measure baseline memory\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        baseline_memory = torch.cuda.memory_allocated() / (1024 ** 2)\n",
        "        print(f\"Baseline GPU memory usage: {baseline_memory:.2f} MB\")\n",
        "\n",
        "    # Load quantized model\n",
        "    print(\"\\nLoading Quantized GPT-2 Model...\")\n",
        "    model_gptq = AutoGPTQForCausalLM.from_quantized(\n",
        "        quantized_model_dir,\n",
        "        use_safetensors=True,\n",
        "        device_map='auto'  # Use 'auto' for proper device mapping\n",
        "    )\n",
        "    torch.cuda.synchronize()\n",
        "    quantized_memory = torch.cuda.memory_allocated() / (1024 ** 2)\n",
        "    print(f\"Quantized model loaded. Memory usage: {quantized_memory:.2f} MB\")\n",
        "\n",
        "    # Load original model\n",
        "    print(\"\\nLoading Original GPT-2 Model...\")\n",
        "    model_original = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
        "    torch.cuda.synchronize()\n",
        "    original_memory = torch.cuda.memory_allocated() / (1024 ** 2)\n",
        "    print(f\"Original model loaded. Memory usage: {original_memory:.2f} MB\")\n",
        "\n",
        "    # Query both models\n",
        "    for i, prompt in enumerate(prompts, 1):\n",
        "        print(f\"\\nPrompt {i}: {prompt}\")\n",
        "\n",
        "        # Query original model\n",
        "        print(\"Original Response:\")\n",
        "        response_original, time_original = query_model(model_original, tokenizer, prompt)\n",
        "        print(f\"Response: {response_original}\")\n",
        "        print(f\"Inference Time: {time_original:.4f} seconds\")\n",
        "\n",
        "        # Query quantized model\n",
        "        print(\"Quantized Response:\")\n",
        "        response_gptq, time_gptq = query_model(model_gptq, tokenizer, prompt)\n",
        "        print(f\"Response: {response_gptq}\")\n",
        "        print(f\"Inference Time: {time_gptq:.4f} seconds\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")\n",
        "    print(\"Please ensure the quantized model exists in 'gpt2-GPTQ' and all dependencies are installed.\")\n",
        "\n",
        "finally:\n",
        "    # Clean up\n",
        "    if 'model_gptq' in locals():\n",
        "        del model_gptq\n",
        "    if 'model_original' in locals():\n",
        "        del model_original\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        import gc\n",
        "        gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dth1SdY5XyS",
        "outputId": "c89d5d28-1031-430c-d4c4-1a74a3aa5ed2"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING - Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\n",
            "WARNING:auto_gptq.modeling._base:Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\n",
            "WARNING - CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n",
            "1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n",
            "2. You are using pytorch without CUDA support.\n",
            "3. CUDA and nvcc are not installed in your device.\n",
            "WARNING:auto_gptq.modeling._base:CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n",
            "1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n",
            "2. You are using pytorch without CUDA support.\n",
            "3. CUDA and nvcc are not installed in your device.\n",
            "WARNING - ignoring unknown parameter in quantize_config.json: quant_method.\n",
            "WARNING:auto_gptq.modeling._base:ignoring unknown parameter in quantize_config.json: quant_method.\n",
            "INFO - The layer lm_head is not quantized.\n",
            "INFO:auto_gptq.modeling._base:The layer lm_head is not quantized.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline GPU memory usage: 185.19 MB\n",
            "\n",
            "Loading Quantized GPT-2 Model...\n",
            "Quantized model loaded. Memory usage: 316.89 MB\n",
            "\n",
            "Loading Original GPT-2 Model...\n",
            "Original model loaded. Memory usage: 805.60 MB\n",
            "\n",
            "Prompt 1: What is the capital of France, and what is its largest city?\n",
            "Original Response:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: What is the capital of France, and what is its largest city?\n",
            "\n",
            "The capital, Paris, is located in the center of the country. It has a population of 1.5 million people. The city is divided into two parts: the north and the south. In the northern part, there is a city called Marseille, which was founded by the French in 1789. There is also the city of Saint-Germain, where there are two cities, the Louvre and St. Peter's Basilica. On the other hand, on the\n",
            "Inference Time: 1.8228 seconds\n",
            "Quantized Response:\n",
            "Response: What is the capital of France, and what is its largest city?\n",
            "\n",
            "The French capital, Paris, has a population of 1.5 million people. It is located in the heart of the French Riviera, which is a UNESCO World Heritage Site. The city is home to a number of museums, including the Louvre, the Museum of Modern Art and the National Gallery of Art in Paris. In addition to the city's museums and galleries, there is also a museum dedicated to France's history and culture, known as the Saint-Germain Museum.\n",
            "Inference Time: 2.6592 seconds\n",
            "\n",
            "Prompt 2: Write a short story about a robot exploring an abandoned city.\n",
            "Original Response:\n",
            "Response: Write a short story about a robot exploring an abandoned city.\n",
            "\n",
            "You can read the full story here: https://www.reddit.com/r/AdviceAnimals/comments/5qk6q9/how_to_make_a_short_story_about_robot_exploring_an_empty_city/\n",
            "Inference Time: 0.8261 seconds\n",
            "Quantized Response:\n",
            "Response: Write a short story about a robot exploring an abandoned city.\n",
            "\n",
            "The story is set in a dystopian future where robots have become the dominant force in the world. The story takes place in an alternate reality where the robots are being used by the government as a means to control the populace. In the story, the protagonist is a young boy who is trying to find a way out of the dystopian world he's living in. He meets a group of robots, who are attempting to take over the city he lives in, and he has to fight them off.\n",
            "Inference Time: 2.8217 seconds\n",
            "\n",
            "Prompt 3: If a car travels 60 miles in 1 hour, how far will it travel in 2.5 hours?\n",
            "Original Response:\n",
            "Response: If a car travels 60 miles in 1 hour, how far will it travel in 2.5 hours?\n",
            "\n",
            "The answer to this question depends on how fast the car is traveling. The speed at which the vehicle is moving will depend on the speed of the road and the distance traveled. For example, if you are driving on a highway, you will be traveling at a rate of 1.4 miles per hour. If you were driving in a city, your speed would be 0.8 mph, and you would have to travel at the same speed to get to your destination. In other words,\n",
            "Inference Time: 1.7964 seconds\n",
            "Quantized Response:\n",
            "Response: If a car travels 60 miles in 1 hour, how far will it travel in 2.5 hours?\n",
            "\n",
            "The answer to this question depends on the speed of the car. The speed at which a vehicle travels is determined by how fast it is traveling at a given speed. For example, if you are driving on a highway, you will be able to travel at speeds of up to 100 miles per hour. If you were driving at 100 mph, then your speed would be 100 km/h, which would mean that you would have traveled at an average of about 1.4 miles/hour.\n",
            "Inference Time: 2.7376 seconds\n",
            "\n",
            "Prompt 4: Explain why the sky appears blue.\n",
            "Original Response:\n",
            "Response: Explain why the sky appears blue.\n",
            "\n",
            "If you're not familiar with the term \"blue sky\", it's a term used to describe a sky that is blue in color. It's also known as a \"red sky\" or an \"orange sky\". The blue sky is the brightest part of the night sky, and the orange sky the darkest part. Blue is a colorless color, which means that it has no visible light source, so it can't be seen by the naked eye. If you look closely, you'll\n",
            "Inference Time: 1.3172 seconds\n",
            "Quantized Response:\n",
            "Response: Explain why the sky appears blue.\n",
            "\n",
            "\"The sky is blue because it's the only place in the world where you can see it. It's also because of the fact that there are so many people who live in it, it makes it so hard for people to get to know each other. The only way to find out what's going on in this world is to go to a place where there's a lot of people, and that's where we're going to be able to make a difference,\" he said. \"\n",
            "Inference Time: 2.6956 seconds\n",
            "\n",
            "Prompt 5: What’s your favorite book, and why?\n",
            "Original Response:\n",
            "Response: What’s your favorite book, and why?\n",
            "\n",
            "I think it's the best book I've read in a long time. I don't know if you've ever read a book like that before, but I think that's one of the things that makes it stand out from the crowd. It's so much more than just a story about a guy who's been through a lot of shit, he's got to go through it again. He's going to have to deal with the fact that his life has changed, that he has to\n",
            "Inference Time: 1.3235 seconds\n",
            "Quantized Response:\n",
            "Response: What’s your favorite book, and why?\n",
            "\n",
            "I think it's the best book I've read in a long time. It's a really good book. I don't know if I'm going to be able to read it again, but I can't wait to get back to reading it.\n",
            "Inference Time: 2.9546 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B8yvtFKL8xdT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}